from lxml import html, etree
import requests
from bs4 import BeautifulSoup
import os
from urllib.parse import urlparse
import shutil
import string


urlbase = "https://dr.library.brocku.ca"


#INSERT THE URL OF THE FULL ITEM RECORD OF AN OBJECT HERE (This will be changed lines from a csv)
webpageLink = "http://dr.library.brocku.ca/handle/10464/14483?show=full"



def build_ftp_folder(page_url):

    #Grabs the page and parses the HTML
    page = requests.get(page_url)
    soup = BeautifulSoup(page.content, 'html.parser')
    
    
    
    #This part makes a folder with the name of the DSpace item
    narrowed_title = "blank"

    for x in soup.find('title'):
        narrowed_title = x.extract()
    
    stripped_title = narrowed_title.translate(str.maketrans('', '', string.punctuation))
    
    os.mkdir(stripped_title)
    
    
    
    #This part grabs the files from the page and downloads them to the folder
    narrowed_file = soup.find_all('div', class_='file-link' )
    link = []
    
    for narrowed_file in narrowed_file:
        link.append(narrowed_file.find('a')['href'])
    
    file_num = 0
    
    for link in link:
        file_num = file_num + 1
        url = (urlbase + link)
        
        paresed_url = urlparse(link)
        parsed_name_of_file = os.path.basename(paresed_url.path)
        
        r = requests.get(url, allow_redirects=True)
        open(parsed_name_of_file, 'wb').write(r.content)
        shutil.move(parsed_name_of_file, stripped_title)


build_ftp_folder(webpageLink)








